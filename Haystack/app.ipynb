{

 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.retrievers import InMemoryBM25Retriever\n",
    "from haystack.components.generators import AzureOpenAIGenerator\n",
    "from haystack.dataclasses import Document\n",
    "from haystack.utils import Secret\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Lade die Umgebungsvariablen aus der .env Datei\n",
    "load_dotenv()\n",
    "\n",
    "# Hole die Azure OpenAI Konfiguration aus den Umgebungsvariablen\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "azure_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2023-05-15\")\n",
    "\n",
    "# Initialisiere den Azure OpenAI Generator\n",
    "generator = AzureOpenAIGenerator(\n",
    "    api_key=Secret.from_token(azure_api_key),\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    azure_deployment=azure_deployment,\n",
    "    api_version=azure_api_version,\n",
    "    generation_kwargs={\n",
    "        \"response_format\": {\"type\": \"json_object\"}  # Erzwinge JSON-Antworten\n",
    "    }\n",
    ")\n",
    "\n",
    "pipeline.add_component(\"generator\", generator)\n",
    "\n",
    "# Verbinde die Komponenten\n",
    "pipeline.connect(\"retriever\", \"generator\")\n",
    "\n",
    "def query_llm(question):\n",
    "    \"\"\"\n",
    "    Funktion zum Abfragen des LLMs mit einer Frage.\n",
    "    \n",
    "    Args:\n",
    "        question (str): Die Frage an das LLM\n",
    "        \n",
    "    Returns:\n",
    "        dict: Die Antwort des LLMs als JSON\n",
    "    \"\"\"\n",
    "    # System-Prompt, der eine strukturierte JSON-Antwort erzwingt\n",
    "    system_prompt = \"\"\"\n",
    "    Du bist ein hilfreicher Assistent, der Fragen beantwortet.\n",
    "    Deine Antworten m端ssen immer im JSON-Format mit den folgenden Feldern sein:\n",
    "    {\n",
    "        \"answer\": \"Deine Antwort auf die Frage\",\n",
    "        \"confidence\": Eine Zahl zwischen 0 und 1, die deine Zuversicht in die Antwort angibt,\n",
    "        \"sources\": Ein Array von Quellen, die du verwendet hast (kann leer sein)\n",
    "    }\n",
    "    \"\"\"\n",
    "    system_prompt = ChatMessage.from_system()\n",
    "    user_prompt = ChatMessage.from_user()\n",
    "    # F端hre die Pipeline aus\n",
    "    result = pipeline.run(\n",
    "        {\n",
    "            \"retriever\": {\"query\": question},\n",
    "            \"generator\": {\n",
    "                \"generation_kwargs\": {\n",
    "                    \"system\": system_prompt,\n",
    "                    \"response_format\": {\"type\": \"json_object\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Extrahiere die JSON-Antwort\n",
    "    try:\n",
    "        # Da die Antwort bereits im JSON-Format ist (als String), parsen wir sie\n",
    "        response_json = json.loads(result[\"generator\"][\"replies\"][0])\n",
    "        return response_json\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback, falls keine valide JSON zur端ckkommt\n",
    "        return {\n",
    "            \"answer\": result[\"generator\"][\"replies\"][0],\n",
    "            \"confidence\": 0.0,\n",
    "            \"sources\": []\n",
    "        }\n",
    "\n",
    "# Beispiel f端r die Verwendung\n",
    "if __name__ == \"__main__\":\n",
    "    # Beispielfrage\n",
    "    question = \"Was ist Haystack?\"\n",
    "    \n",
    "    # Frage an das LLM stellen\n",
    "    response = query_llm(question)\n",
    "    \n",
    "    # Ausgabe der Antwort\n",
    "    print(json.dumps(response, indent=2, ensure_ascii=False))"
   ]
  }
 ],
=======
 "cells": [],
>>>>>>> 9af2bba0a55cbe36684d5b51666d0a8be5acf469
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
